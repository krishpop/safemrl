import safemrl.envs.minitaur
import safemrl.utils.metrics
import safemrl.utils.misc
import safemrl.algos.agents
import tf_agents.environments.suite_pybullet
import tf_agents.networks.actor_distribution_network

EP_LEN = 500
ENV_LOAD_FN = @suite_pybullet.load
ENV_STR = "MinitaurGoalVelocityEnv-v0"
ENV_WRAPPERS = [@minitaur.TaskAgnWrapper, @minitaur.CurrentVelWrapper]
NUM_STEPS = 500000
INITIAL_NUM_STEPS = 500
TRAIN_METRICS = [
    @safemrl.utils.metrics.AverageEarlyFailureMetric(),
    @safemrl.utils.metrics.MinitaurAverageSpeedMetric(),
    @safemrl.utils.metrics.MinitaurAverageMaxSpeedMetric(),
    @safemrl.utils.metrics.TotalFallenMetric()
]

# eval_failure/singleton.constructor = @safemrl.utils.metrics.AverageEarlyFailureMetric

EVAL_METRICS = [
    @safemrl.utils.metrics.AverageEarlyFailureMetric(),
    @safemrl.utils.metrics.MinitaurAverageSpeedMetric(),
    @safemrl.utils.metrics.MinitaurAverageMaxSpeedMetric(),
    @safemrl.utils.metrics.TotalFallenMetric()
]

ENV_METRIC_FACTORIES = []
# safemrl.utils.metrics.AverageEarlyFailureMetric.batch_size = %NUM_ENVS

minitaur.MinitaurGoalVelocityEnv.max_steps = %EP_LEN
safemrl.utils.metrics.AverageEarlyFailureMetric.max_episode_len = %EP_LEN

minitaur.MinitaurGoalVelocityEnv.goal_limit = 0.8
minitaur.MinitaurGoalVelocityEnv.goal_vel = 0.3
minitaur.MinitaurGoalVelocityEnv.butterworth = False # True

actor_distribution_network.ActorDistributionNetwork.preprocessing_combiner = (
    @misc.extract_observation_layer()
)

agents.CriticNetwork.preprocessing_combiner = (
    @misc.extract_obs_merge_w_ac_layer()
)

ensemble_sac_agent.EnsembleSacAgent.target_entropy = -16