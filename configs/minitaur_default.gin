import safemrl.envs.minitaur
import safemrl.utils.metrics
import safemrl.algos.agents
import tf_agents.environments.suite_pybullet
import tf_agents.networks.actor_distribution_network

EP_LEN = 500
ENV_STR = "MinitaurGoalVelocityEnv-v0"
ENV_LOAD_FN = @suite_pybullet.load
TRAIN_METRICS = [
    @safemrl.utils.metrics.AverageEarlyFailureMetric()
]

eval_failure/singleton.constructor = @safemrl.utils.metrics.AverageEarlyFailureMetric
EVAL_METRICS = [
               @eval_failure/singleton()
]
# safemrl.utils.metrics.AverageEarlyFailureMetric.batch_size = %NUM_ENVS

suite_pybullet.load.gym_env_wrappers = (@minitaur.TaskAgnWrapper,)
minitaur.MinitaurGoalVelocityEnv.max_steps = %EP_LEN
minitaur.MinitaurGoalVelocityEnv.accurate_motor_model_enabled = True
minitaur.MinitaurGoalVelocityEnv.never_terminate = False
minitaur.MinitaurGoalVelocityEnv.history_length = 5
minitaur.MinitaurGoalVelocityEnv.urdf_version = "rainbow_dash_v0"
minitaur.MinitaurGoalVelocityEnv.history_include_actions = True
minitaur.MinitaurGoalVelocityEnv.control_time_step = 0.02
minitaur.MinitaurGoalVelocityEnv.history_include_states = True
minitaur.MinitaurGoalVelocityEnv.include_leg_model = True

minitaur.MinitaurGoalVelocityEnv.goal_limit = 0.8
minitaur.MinitaurGoalVelocityEnv.goal_vel = 0.4
minitaur.MinitaurGoalVelocityEnv.butterworth = True

actor_distribution_network.ActorDistributionNetwork.preprocessing_combiner = (
    @agents.extract_observation_layer()
)

agents.CriticNetwork.preprocessing_combiner = (
    @agents.extract_obs_merge_w_ac_layer()
)