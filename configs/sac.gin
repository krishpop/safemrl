import gin.tf.external_configurables
import tensorflow
import tf_agents.agents.sac.sac_agent
import tf_agents.environments.suite_pybullet
import tf_agents.agents.ddpg.critic_network
import tf_agents.networks.actor_distribution_network

import safemrl.trainer
import safemrl.algos.ensemble_sac_agent

include 'networks.gin'

NUM_ENVS = 1
NUM_EVAL = 30
NUM_STEPS = 1000000
LEARNING_RATE = 3e-4
SLOW_LEARNING_RATE = 1e-5
AGENT_CLASS = @sac_agent.SacAgent

trainer.train_eval.env_name = %ENV_STR
trainer.train_eval.env_load_fn = %ENV_LOAD_FN
trainer.train_eval.agent_class = %AGENT_CLASS
trainer.train_eval.train_metrics = %TRAIN_METRICS
trainer.train_eval.eval_metrics = %EVAL_METRICS
trainer.train_eval.num_global_steps = %NUM_STEPS
trainer.train_eval.online_critic = False
trainer.train_eval.keep_rb_checkpoint = False
trainer.train_eval.debug_summaries = True

trainer.train_eval.initial_collect_driver_class = @init_collect/dynamic_step_driver.DynamicStepDriver
init_collect/dynamic_step_driver.DynamicStepDriver.num_steps = %EP_LEN
trainer.train_eval.collect_driver_class = @collect_driver/dynamic_step_driver.DynamicStepDriver
collect_driver/dynamic_step_driver.DynamicStepDriver.num_steps = 1

agents.normal_projection_net.scale_distribution = True

sac_agent.SacAgent.target_update_tau = 0.005
sac_agent.SacAgent.reward_scale_factor = 1.0
sac_agent.SacAgent.actor_optimizer = @ac_opt/tf.keras.optimizers.Adam()
ac_opt/tf.keras.optimizers.Adam.learning_rate = %LEARNING_RATE
sac_agent.SacAgent.critic_optimizer = @cr_opt/tf.keras.optimizers.Adam()
cr_opt/tf.keras.optimizers.Adam.learning_rate = %LEARNING_RATE
sac_agent.SacAgent.alpha_optimizer = @al_opt/tf.keras.optimizers.Adam()
al_opt/tf.keras.optimizers.Adam.learning_rate = %LEARNING_RATE

# train_eval_ensemble.train_eval.gradient_clipping = 1.
# train_eval_ensemble.train_eval.actor_learning_rate = %SLOW_LEARNING_RATE

# sac.sac_agent.actor_net = @actor_distribution_network.ActorDistributionNetwork()
