import gin.tf.external_configurables
import tf_agents.agents.sac.sac_agent
import tf_agents.environments.suite_pybullet
import tf_agents.agents.ddpg.critic_network
import tf_agents.networks.actor_distribution_network

import safemrl.trainer
import safemrl.algos.wcpg_agent

include 'networks.gin'
include 'sac.gin'

LEARNING_RATE = 3e-4
CRITIC_LR = 1e-4
AGENT_CLASS = 'wcpg'
REWARD_SCALE_FACTOR = 10.
GRADIENT_CLIPPING = 2.0
# ENV_WRAPPERS = []

TRAIN_METRICS = [
    @safemrl.utils.metrics.AverageEarlyFailureMetric()]
EVAL_METRICS = [
    @safemrl.utils.metrics.AverageEarlyFailureMetric()]

# trainer.train_eval.gym_env_wrappers = []

wcpg_agent.WcpgAgent.target_update_tau = 0.005
wcpg_agent.WcpgAgent.reward_scale_factor = %REWARD_SCALE_FACTOR
# wcpg_agent.WcpgAgent.gradient_clipping = %GRADIENT_CLIPPING
wcpg_agent.WcpgAgent.gamma = 0.99
wcpg_agent.WcpgAgent.exploration_noise_stddev = 1.0
wcpg_agent.WcpgAgent.actor_optimizer = @ac_opt/tf.keras.optimizers.Adam()
ac_opt/tf.keras.optimizers.Adam.learning_rate = %LEARNING_RATE
wcpg_agent.WcpgAgent.critic_optimizer = @cr_opt/tf.keras.optimizers.Adam()
cr_opt/tf.keras.optimizers.Adam.learning_rate = %CRITIC_LR

# agents.WcpgActorNetwork.fc_layer_params = (32, )
# agents.WcpgActorNetwork.preprocessing_layer_size = 32
# agents.DistributionalCriticNetwork.joint_fc_layer_params = (64, )
# agents.DistributionalCriticNetwork.preprocessing_layer_size = 64
